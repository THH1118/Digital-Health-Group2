# -*- coding: utf-8 -*-
"""hbs_code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K69cZzWSVX0wnFrao-xiIwfSn_31PrNU
"""

# ===========================================
# ÂÆåÊï¥ÁâàÔºöTwo-Stage + Ê©üÁéáÊ†°Ê≠£ + P value
# ===========================================
!pip install -U imbalanced-learn xgboost scikit-learn -q

import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import xgboost as xgb

from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    roc_auc_score, roc_curve, accuracy_score, precision_score,
    recall_score, f1_score, classification_report
)
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.calibration import calibration_curve, CalibratedClassifierCV

from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.ensemble import EasyEnsembleClassifier
cat_cols = ["sex", "hd or capd", "dm", "htn", "outop", "thyroidectomy", "thymectomy"]

# ----------------------- ÂÖ¨Áî®ÂáΩÂºè -----------------------
def make_calibrated(model, seed=42, method='sigmoid'):
    """Êää‰ªª‰∏Ä pipeline ÂåÖË£ùÊàêÂèØÊ†°Ê≠£ÁöÑÂàÜÈ°ûÂô® (Platt scaling)„ÄÇ"""
    return CalibratedClassifierCV(estimator=model, method=method, cv=5, n_jobs=-1)


def bootstrap_auc_ci(y, proba, n_boot=1000, seed=42):
    """Ëá™Âä©Ê≥ïË®àÁÆó AUROC 95% CIÔºå‰∏¶ÂõûÂÇ≥ bootstrap ÂàÜÂ∏É„ÄÇ"""
    rng = np.random.default_rng(seed)
    aucs = []
    for _ in range(n_boot):
        idx = rng.integers(0, len(y), len(y))
        if len(np.unique(y[idx])) < 2:
            continue
        aucs.append(roc_auc_score(y[idx], proba[idx]))
    return np.percentile(aucs, [2.5, 97.5]), np.array(aucs)


def p_value_vs_baseline(boot_model, boot_base):
    """ÈõôÂ∞æÊ™¢ÂÆöÔºöÊØîËºÉ AUROC Ëàá baseline (Logistic) Â∑ÆÁï∞ÁöÑ p value„ÄÇ"""
    diff = boot_model - boot_base
    p = 2 * min((diff <= 0).mean(), (diff >= 0).mean())
    return p


def best_cutoff(y, proba):
    """Youden index ÊâæÊúÄ‰Ω≥ cut-offÔºå‰∏¶ÂõûÂÇ≥ÂêÑÊåáÊ®ô + classification report„ÄÇ"""
    fpr, tpr, thr = roc_curve(y, proba)
    idx = np.argmax(tpr - fpr)
    cut = thr[idx]
    y_pred = (proba >= cut).astype(int)
    metrics = {
        'cut-off': cut,
        'accuracy': accuracy_score(y, y_pred),
        'precision': precision_score(y, y_pred),
        'recall': recall_score(y, y_pred),
        'f1-score': f1_score(y, y_pred),
        'specificity': 1 - fpr[idx]
    }
    report = classification_report(y, y_pred, digits=3)
    return metrics, report


from scipy.stats import weibull_min

def reliability_plot(ax, y, proba, name, bins=10, seed=42):
    prob_true, prob_pred = calibration_curve(y, proba, n_bins=bins)
    ax.plot(prob_pred, prob_true, marker="o", label=name)

    rng = np.random.default_rng(seed)
    boot_probs = []
    for _ in range(500):
        idx = rng.integers(0, len(y), len(y))
        if len(np.unique(y[idx])) < 2:
            continue
        pt, _ = calibration_curve(y[idx], proba[idx], n_bins=bins)
        if len(pt) == len(prob_pred):
            boot_probs.append(pt)

    boot_probs = np.array(boot_probs)
    if len(boot_probs) > 0:
        ci_low, ci_high = np.percentile(boot_probs, [2.5, 97.5], axis=0)
        ax.fill_between(prob_pred, ci_low, ci_high, color='gray', alpha=0.2, label='95% CI')

    # Weibull Êì¨Âêà
    c, loc, scale = weibull_min.fit(proba, floc=0)
    x = np.linspace(0, 1, 100)
    ax.plot(x, weibull_min.cdf(x, c, loc=loc, scale=scale),
            linestyle='--', color='green', label='Weibull Fit')

    ax.plot([0, 1], [0, 1], linestyle='--', color='grey')
    ax.set_title(f"Reliability ‚Äì {name}")
    ax.set_xlabel("Predicted Probability")
    ax.set_ylabel("Observed Probability")
    ax.legend()


# ----------------------- Two-Stage ÂâçËôïÁêÜ -----------------------
def prepare_data(df, target, long, short, params, cat_cols, seed=42):
    df_processed = df[long + short + [target]].copy()
    cat_cols = ["sex", "hd or capd", "dm", "htn", "outop", "thyroidectomy", "thymectomy"]
    # Áº∫ÂÄºÂ°´Ë£úÁ≠ñÁï•
    for col in df_processed.columns:
        if col in cat_cols:
            df_processed[col].fillna(df_processed[col].mode()[0], inplace=True)
        else:
            df_processed[col].fillna(df_processed[col].mean(), inplace=True)

    X, y = df_processed.drop(columns=[target]), df_processed[target]
    X_tr, X_te, y_tr, y_te = train_test_split(X, y, stratify=y, test_size=0.2, random_state=seed)

    X_tr_long, y_tr_long = SMOTE(random_state=seed).fit_resample(X_tr[long], y_tr)

    xgb1 = xgb.XGBClassifier(**params, subsample=0.8,
                             colsample_bytree=0.8, eval_metric="auc", random_state=seed)
    xgb1.fit(X_tr_long, y_tr_long)

    risk_tr = xgb1.predict_proba(X_tr[long])[:, 1]
    risk_te = xgb1.predict_proba(X_te[long])[:, 1]

    X_tr2 = pd.concat([X_tr[short].reset_index(drop=True),
                       pd.Series(risk_tr, name="risk_score")], axis=1)
    X_te2 = pd.concat([X_te[short].reset_index(drop=True),
                       pd.Series(risk_te, name="risk_score")], axis=1)

    return X_tr2, X_te2, y_tr.reset_index(drop=True), y_te.reset_index(drop=True)



# ----------------------- ‰∏ªÁ®ãÂºè -----------------------
def run_all_models(df, target, long, short, stage1_params, task_name, seed=42,cat_cols=cat_cols):
    X_tr, X_te, y_tr, y_te = prepare_data(df, target, long, short, stage1_params, seed)

    models = {
        'Logistic': ImbPipeline([
            ('smote', SMOTE(random_state=seed)),
            ('scaler', StandardScaler()),
            ('clf', LogisticRegression(max_iter=1000, class_weight='balanced', random_state=seed))
        ]),
        'SVM (cal)': make_calibrated(
            ImbPipeline([
                ('smote', SMOTE(random_state=seed)),
                ('scaler', StandardScaler()),
                ('clf', SVC(probability=True, class_weight='balanced', random_state=seed))
            ]), seed
        ),
        'kNN (cal)': make_calibrated(
            ImbPipeline([
                ('smote', SMOTE(random_state=seed)),
                ('scaler', StandardScaler()),
                ('clf', KNeighborsClassifier(n_neighbors=5))
            ]), seed
        ),
        'EasyEnsemble (cal)': make_calibrated(
            EasyEnsembleClassifier(n_estimators=30, random_state=seed), seed
        ),
        'Two-stage XGB (cal)': make_calibrated(
            ImbPipeline([
                ('smote', SMOTE(random_state=seed)),
                ('clf', xgb.XGBClassifier(**stage1_params,
                                          subsample=0.8, colsample_bytree=0.8,
                                          eval_metric='auc', random_state=seed))
            ]), seed
        ),
    }

    fig, ax = plt.subplots(figsize=(6, 6))
    fig2, axs2 = plt.subplots(1, len(models), figsize=(4 * len(models), 4))

    results = {}
    # ---------- Ë®ìÁ∑¥ & Ë©ï‰º∞ ----------
    for i, (name, model) in enumerate(models.items()):
        model.fit(X_tr, y_tr)
        proba = model.predict_proba(X_te)[:, 1]
        auc_val = roc_auc_score(y_te, proba)
        (ci_low, ci_high), boot = bootstrap_auc_ci(y_te.values, proba)
        metrics, report = best_cutoff(y_te, proba)
        results[name] = {
            'auc': auc_val,
            'ci': (ci_low, ci_high),
            'boot': boot,
            **metrics
        }

        # ROC
        fpr, tpr, _ = roc_curve(y_te, proba)
        ax.plot(fpr, tpr, label=f'{name} (AUC={auc_val:.3f})')

        # Reliability
        reliability_plot(axs2[i], y_te, proba, name)

        # Console report
        print(f'\nüîç [{name}] classification report:\n{report}')

    # ---------- P value vs Logistic ----------
    base_boot = results['Logistic']['boot']
    for name, info in results.items():
        if name == 'Logistic':
            info['p'] = 'reference'
        else:
            info['p'] = p_value_vs_baseline(info['boot'], base_boot)

    # ---------- ÂúñË°® ----------
    ax.plot([0, 1], [0, 1], ls='--', color='grey')
    ax.set_title(f'ROC ‚Äì {task_name}')
    ax.set_xlabel('False Positive Rate')
    ax.set_ylabel('True Positive Rate')
    ax.legend()
    plt.tight_layout()
    plt.show()
    plt.tight_layout()
    plt.show()

    # ---------- ÁµêÊûúË°® ----------
    rows = []
    for name, info in results.items():
        ci_l, ci_h = info['ci']
        rows.append({
            'Model': name,
            'AUROC (95% CI)': f'{info["auc"]:.3f} ({ci_l:.3f}-{ci_h:.3f})',
            'cut-off': f'{info["cut-off"]:.3f}',
            'accuracy': f'{info["accuracy"]:.3f}',
            'precision': f'{info["precision"]:.3f}',
            'recall': f'{info["recall"]:.3f}',
            'f1-score': f'{info["f1-score"]:.3f}',
            'specificity': f'{info["specificity"]:.3f}',
            'P value': info['p']
        })
    res_df = pd.DataFrame(rows).sort_values('AUROC (95% CI)', ascending=False).reset_index(drop=True)
    print('\nüîñ ÊúÄÁµÇÁµ±Ë®àÊëòË¶ÅË°®:\n', res_df)


# ----------------------- Ë∑ë HBS Á§∫ÁØÑ -----------------------
hbs_path = "/content/drive/MyDrive/digital health project/hbsdata_noothers_cleaned_FINAL0513.xlsx"
hbs_df = pd.read_excel(hbs_path)
#sex dm htn
long_cols = ["age", "sex", "bmi", "esrd yrs", "bw","dm", "htn",
             "prealk", "preca", "prep", "prepth", "prehb",
             "preca √ó prep", "prepth / esrd yrs"]
short_cols = ["outop", "op time √ó blood loss",
              "total_size", "total_weight", "thyroidectomy"]

run_all_models(
    hbs_df, 'hbs',
    long_cols, short_cols,
    stage1_params={'n_estimators': 300, 'learning_rate': 0.01,
                   'max_depth': 3, 'scale_pos_weight': 0.5},
    task_name='HBS'
    ,cat_cols=cat_cols
)